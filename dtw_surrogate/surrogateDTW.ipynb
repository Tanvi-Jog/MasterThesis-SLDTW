{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"surrogateDTW.ipynb","provenance":[],"collapsed_sections":["UHtURblheKxm","mFv9zGBQeFZo","M37o60Q9e8wF","bSvk2l0kfSXy"],"authorship_tag":"ABX9TyMLad0YdpblWVPzXQHUt5Uk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TeuWGcWONfnv"},"outputs":[],"source":["%pip install tslearn tsai"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import random\n","import time\n","import torch.optim as optim\n","import os\n","import matplotlib.pyplot as plt\n","import json\n","\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","from torch.nn.utils import weight_norm\n","from torch.cuda import device_count\n","from soft_dtw_cuda import SoftDTW\n","from tslearn.metrics import dtw, dtw_path\n","from tsai.imports import *\n","from tsai.utils import *\n","from tsai.models.layers import *\n","from tsai.models.utils import *\n","from datetime import datetime"],"metadata":{"id":"4sPlEuJ9NliT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"gdH4PO4MNo4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["randomlist = [35, 73, 86, 7, 98] "],"metadata":{"id":"oJftMp8DVgxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for reproducibility\n","seed = randomlist[0]\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"],"metadata":{"id":"01tyoOSKVqEx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Prepreration"],"metadata":{"id":"VAKrvFa0d6jK"}},{"cell_type":"code","source":["class dataManager:\n","  # the constructor loads the data\n","  def __init__(self, data_path, data_set, train_size, valid_size):\n","    self.dataset = data_set\n","    self.datasets = self.preprocess(data_path, train_size, valid_size)\n","\n","  def preprocess(self, data_path, train_size, valid_size):\n","    # load the data\n","    data = self.load_data(data_path)\n","    # normalize the data\n","    if self.dataset == 'cost_data' or self.dataset == 'ecg' or self.dataset == 'synthetic':\n","        norm_type = 0\n","    else:\n","        norm_type = 2\n","    n_data = self.normalized(norm_type, data)\n","    # split the data into train/valid/test sets\n","    if self.dataset == 'electricity' or self.dataset == 'traffic':\n","        n, m = n_data.shape\n","    else:\n","        n, m, d = n_data.shape\n","    train_indices = int(n * train_size)\n","    valid_indices = int((train_size+valid_size) * n)\n","    test_indices = n - valid_indices\n","    train_data = np.array(n_data[0:train_indices])\n","    valid_data = np.array(n_data[train_indices:valid_indices])\n","    test_data = np.array(n_data[valid_indices:n]) \n","    # convert the data into supervised problem of x,y\n","    data_sets = {}\n","    data_sets['train'], data_sets['valid'], data_sets['test'] = self.generate_data(self, self.dataset,train_data, valid_data, test_data)\n","    return data_sets\n","\n","  def parse_ucr(self, filename):\n","    X = []\n","    for line in open(filename):\n","      line = line.strip()\n","      arr = line.split(\",\")\n","      f_line = list(map(float, arr[1:]))\n","      f_line = np.array(f_line).reshape(-1, 1)\n","      X.append(f_line)\n","    return X\n","\n","  def load_data(self, data_path):\n","    if self.dataset == 'electricity':\n","      data_period = [2014]\n","      df = pd.read_csv(data_path, sep=';', decimal=',')\n","      df.rename(columns = {\"Unnamed: 0\": \"time\"}, inplace = True)\n","      df['time'] = pd.to_datetime(df['time'])\n","      df_included = df[df[\"time\"].dt.year.isin(data_period)].reset_index()\n","      df_included.drop('index', axis=1, inplace=True)\n","      #hourly level aggregation\n","      df_included.index = pd.to_datetime(df_included['time'])\n","      df_included = df_included.resample('1h').mean()\n","      total_data = np.array([df_included.loc[i,:] for i in df_included.index])\n","\n","    if self.dataset == 'traffic':\n","      f = open(data_path)\n","      total_data = np.loadtxt(f, delimiter=',')\n","  \n","    if self.dataset == 'ecg' or self.dataset == 'synthetic':\n","        data_folder = 'synthetic_control' if self.dataset == 'synthetic' else 'ECG5000'\n","        tr = os.path.join(data_path, \"%s_TRAIN\" % data_folder)\n","        te = os.path.join(data_path, \"%s_TEST\" % data_folder)\n","        X_tr = np.array(self.parse_ucr(tr))\n","        X_te = np.array(self.parse_ucr(te))\n","        total_data = np.concatenate((X_tr, X_te), axis=0)\n","    return total_data\n","  \n","  def normalized(self, normalize, data):\n","    dat = np.zeros(data.shape)\n","    n, m = data.shape\n","    if (normalize == 0):\n","        dat = data\n","    if (normalize == 1): # normalized by the maximum value of entire matrix.\n","        dat = data / np.max(data)  \n","    if (normalize == 2): # normlized by the maximum value of each row(sensor).\n","      for i in range(m):\n","        dat[:,i] = data[:,i] / np.max(np.abs(data[:,i]))\n","    return dat\n","  \n","  def split_series(self, data):\n","    P, horizon = 168, 24\n","    n_len, m = data.shape\n","    X = np.ndarray((n_len, P, m))\n","    Y = np.ndarray((n_len, m))\n","\n","    for i in range(n_len):\n","      start = i + P\n","      end = (start+1) + horizon\n","      if end < n_len:\n","          X[i,:,:] = torch.from_numpy(data[i:start, :])\n","          Y[i,:] = torch.from_numpy(data[end, :])\n","      else:\n","          break\n","    return np.float32(X), np.float32(Y)\n","  \n","  def generate_data(self, dataset, train_data, valid_data, test_data):\n","    if dataset == 'ecg' or dataset == 'synthetic':\n","      proportion=0.6\n","      len_ts = train_data.shape[1]\n","      len_input = int(round(len_ts * proportion))\n","      len_output = len_ts - len_input\n","      x_train = np.float32(train_data[:, :len_input, :])\n","      y_train = np.float32(train_data[:, len_input:, 0])\n","      x_valid = np.float32(valid_data[:, :len_input, :])\n","      y_valid = np.float32(valid_data[:, len_input:, 0])\n","      x_test = np.float32(test_data[:, :len_input, :])\n","      y_test = np.float32(test_data[:, len_input:, 0])\n","\n","    if dataset == 'electricity' or dataset == 'traffic':\n","      x_train, y_train = self.split_series(train_data)\n","      x_valid, y_valid = self.split_series(valid_data)\n","      x_test, y_test = self.split_series(test_data)\n","    \n","    return [x_train, y_train], [x_valid, y_valid], [x_test, y_test]"],"metadata":{"id":"P2AHsDW9Vsp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path ='./datasets/traffic.txt'\n","data_sets = ['ecg', 'synthetic', 'electricity', 'traffic']\n","data_set = data_sets[0]\n","ds = dataManager(data_path, data_set)\n","# Load Data \n","trainX, trainY = ds.datasets['train'][0], ds.datasets['train'][1]\n","validX, validY = ds.datasets['valid'][0], ds.datasets['valid'][1]\n","testX, testY = ds.datasets['test'][0], ds.datasets['test'][1]"],"metadata":{"id":"8lHT9SqDX2YT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_worker(worker_id):\n","  worker_seed = torch.initial_seed() % 2**32\n","  np.random.seed(worker_seed)\n","  random.seed(worker_seed)\n","g = torch.Generator()\n","g.manual_seed(seed)\n","\n","# Load Data\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device)\n","\n","print('trainX, trainY', trainX.shape, trainY.shape)\n","print('validX, validY', validX.shape, validY.shape)\n","print('testX, testY', testX.shape, testY.shape)\n","\n","workers = 2 * device_count() \n","batch_size = 128\n","\n","datasets = {'train':  TensorDataset(trainX, trainY), 'val': TensorDataset(validX, validY), 'test': TensorDataset(testX, testY)}\n","dataloaders_dict = {}\n","for x in ['train', 'val', 'test']: \n","  if x == 'train': \n","    dataloaders_dict[x] = DataLoader(datasets[x], batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=workers, worker_init_fn=seed_worker, generator=g)\n","  else:\n","    dataloaders_dict[x] = DataLoader(datasets[x], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True, num_workers=workers, worker_init_fn=seed_worker, generator=g)\n"],"metadata":{"id":"QbKRt-QjWqjX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Configure the dataset and models here."],"metadata":{"id":"cozSjhIGegcl"}},{"cell_type":"code","source":["nets = ['mlp', 'tcn']\n","loss_types = ['mse', 'soft_dtw', 'dtw_surrogate']\n","\n","model_type = nets[0]\n","loss = loss_types[0]"],"metadata":{"id":"NGu7RkdkYiQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if model_type == 'mlp':\n","  N_input = trainX.shape[1] * trainX.shape[2]\n","  N_output = trainY.shape[1]\n","if model_type == 'tcn':\n","  N_input =  trainX.shape[1]\n","  N_output = trainY.shape[1]\n","print('N_input, N_output', N_input, N_output)"],"metadata":{"id":"rTZuqOEqXsoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Network classes and helper functions"],"metadata":{"id":"UHtURblheKxm"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","  '''\n","  This class represents the MLP model used for prediction.\n","  '''\n","  def __init__(self, input_features, output_features, hidden_units): \n","    super(MLP, self).__init__()\n","    self.hidden = nn.Linear(input_features, hidden_units)\n","    self.fc = nn.Linear(hidden_units, output_features)\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, x):\n","    x = self.hidden(x)\n","    x = self.relu(x)\n","    x = self.fc(x)\n","    return x"],"metadata":{"id":"SXTl1d8vWzOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TemporalBlock(Module):\n","  def __init__(self, ni, nf, ks, stride, dilation, padding, dropout=0.):\n","    self.conv1 =  weight_norm(nn.Conv1d(ni,nf,ks,stride=stride,padding=padding,dilation=dilation))\n","    self.relu1 = nn.ReLU()\n","    self.dropout1 = nn.Dropout(dropout)\n","\n","    self.conv2 =  weight_norm(nn.Conv1d(nf,nf,ks,stride=stride,padding=padding,dilation=dilation))\n","    self.relu2 = nn.ReLU()\n","    self.dropout2 = nn.Dropout(dropout)\n","\n","    self.conv3 =  weight_norm(nn.Conv1d(nf,nf,ks,stride=stride,padding=padding,dilation=dilation))\n","    self.relu3 = nn.ReLU()\n","    self.dropout3 = nn.Dropout(dropout)\n","\n","    self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1,\n","                              self.conv2, self.relu2, self.dropout2,\n","                              self.conv3, self.relu3, self.dropout3) #, self.chomp1, self.chomp2, self.chomp3\n","    self.relu = nn.ReLU()\n","    self.init_weights()\n","\n","  def init_weights(self):\n","    # kaiming initialization\n","    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n","    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n","    nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n","\n","  def forward(self, x):\n","    out = self.net(x)\n","    return self.relu(out)\n","\n","def TemporalConvNet(c_in, layers, ks=2, dropout=0.):\n","  temp_layers = []\n","  for i in range(len(layers)):\n","    dilation_size = 2 ** i\n","    ni = c_in if i == 0 else layers[i-1]\n","    nf = layers[i]\n","    temp_layers += [TemporalBlock(ni, nf, ks, stride=1, dilation=dilation_size, padding=(ks-1) * dilation_size, dropout=dropout)]\n","  return nn.Sequential(*temp_layers)\n","\n","\n","class TCN(Module):\n","  '''\n","  This class represents the TCN model used for prediction.\n","  '''\n","  def __init__(self, c_in, c_out, layers=[128, 64, 32], ks=7, conv_dropout=0., fc_dropout=0.):\n","    self.tcn = TemporalConvNet(c_in, layers, ks=ks, dropout=conv_dropout)\n","    self.gap = GAP1d()\n","    self.dropout = nn.Dropout(fc_dropout) if fc_dropout else None\n","    self.linear = nn.Linear(layers[-1],c_out)\n","    self.init_weights()\n","\n","  def init_weights(self):\n","    nn.init.kaiming_normal_(self.linear.weight, mode='fan_in', nonlinearity='relu')\n","\n","  def forward(self, x):\n","    x = self.tcn(x)\n","    x = self.gap(x)\n","    if self.dropout is not None: x = self.dropout(x)\n","    return self.linear(x)"],"metadata":{"id":"frzPH2HrW4JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SurrogateNetwork(nn.Module):\n","  '''\n","  This class represents the surrogate DTW model used for loss function.\n","  '''\n","  def __init__(self, dataset_name, batch_size): \n","    super().__init__()\n","    self.input_dim = 1 \n","    self.output_dim = 1\n","    self.horizon = {'ecg': 56, 'synthetic': 24, 'traffic_data': 862, 'electricity': 370 }[dataset_name]\n","    self.bs = batch_size\n","    self.conv_layer = nn.Sequential(\n","      nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3),\n","      nn.BatchNorm1d(64),\n","      nn.ReLU(),\n","      nn.Conv1d(64, 32, 3, dilation=2),\n","      nn.BatchNorm1d(32),\n","      nn.ReLU(),\n","      nn.Conv1d(32, 16, 3),\n","      nn.BatchNorm1d(16),\n","      nn.ReLU(),\n","      nn.Conv1d(16, 8, 3),\n","      nn.ReLU()\n","    )\n","    self.fc1 = nn.Linear({56: 368, 24:112, 66:448, 862:6816, 370:2880}[self.horizon], 1) \n","    \n","    # If L1 norm without built-in function\n","    self.fcOut = nn.Linear(batch_size, 1)\n","    \n","    # If L1 norm with built-in function\n","    #self.l1 = nn.L1Loss()\n","  \n","  def forward(self, x1, x2):\n","    x1 = torch.transpose(x1, 1, 2)\n","    x1 = self.conv_layer(x1)\n","    x1 = x1.reshape(x1.size(0), -1)\n","    x1 = self.fc1(x1) \n","\n","    x2 = torch.transpose(x2, 1, 2)\n","    x2 = self.conv_layer(x2)\n","    x2 = x2.reshape(x2.size(0), -1)\n","    x2 = self.fc1(x2)\n","    \n","    # L1 norm without built-in function\n","    x = torch.abs(x1 - x2)\n","    x = x.permute(1, 0)\n","    x = self.fcOut(x)\n","\n","    # L1 norm with built-in function\n","    #x = self.l1(x1, x2)\n","    return x"],"metadata":{"id":"ChvW7K4JXBLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Record():\n","  def __init__(self):\n","    self.metric_criterion = nn.MSELoss()\n","  \n","  # calculate the metrics here\n","  def update(self, output, y):\n","    loss_mse = torch.tensor(0)\n","    # MSE    \n","    loss_mse = self.metric_criterion(output, y)   \n","    # DTW\n","    loss_dtw = 0 \n","    for k in range(batch_size):         \n","      target_k_cpu = y[k,:,0:1].view(-1).detach().cpu().numpy()\n","      output_k_cpu = output[k,:,0:1].view(-1).detach().cpu().numpy()\n","      sim = dtw(target_k_cpu, output_k_cpu)   \n","      loss_dtw += sim\n","    loss_dtw = loss_dtw /batch_size\n","    return loss_mse.item(), loss_dtw"],"metadata":{"id":"22-4oai6XKud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper functions\n","def plot_losses(plot_values_1, plot_values_2, plt_name, label_1, label_2, plt_title):\n","  plt.plot(plot_values_1, label=label_1)\n","  plt.plot(plot_values_2, label=label_2)\n","  plt.legend()\n","  plt.title(plt_title)\n","  plt.savefig(f\"plots/{plt_name}.png\")\n","  # plt.show()\n","  plt.close()\n","\n","def MetaLoss(my_outputs, my_labels):\n","  criteria = nn.L1Loss()\n","  m_loss = criteria(my_outputs, my_labels)\n","  return m_loss\n","\n","def set_parameter_requires_grad_true(model):\n","  for param in model.parameters():\n","    param.requires_grad = True\n","  return model\n","\n","def set_parameter_requires_grad_false(model):\n","  for param in model.parameters():\n","    param.requires_grad = False\n","  return model\n","\n","def format_variables(var_1, var_2, batch_size): \n","  '''\n","  var_1 == y\n","  var_2 == y_hat\n","  '''\n","  y = var_1.view([batch_size, -1, 1])\n","  y_hat = var_2.view([batch_size, -1, 1])\n","  return y, y_hat"],"metadata":{"id":"i52coCX-XN2u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training routines"],"metadata":{"id":"mFv9zGBQeFZo"}},{"cell_type":"code","source":["import copy\n","def train_model(model, dataloaders, criterion, optimizer, batch_size, loss_type='dtw_surrogate', num_epochs=25):\n","  since = time.time()\n","  loss_history = {'train loss':list(), 'test loss':list()} \n","  best_loss = float(\"inf\")\n","  best_model_wts = copy.deepcopy(model.state_dict())\n","  record = Record()\n","  title = f\"{data_set}_prediction_model_{loss_type}_{model_type}\"\n","  model = model.to(device)\n","\n","  for epoch in range(num_epochs):\n","    print('\\n Epoch {}/{}'.format(epoch, num_epochs - 1))\n","    print('-' * 10)\n","    # Training\n","    phase = 'train'\n","    model.train()\n","    running_train_loss = []\n","    for inputs, targets in dataloaders[phase]:\n","      if model_type == 'mlp':\n","        inputs = inputs.reshape((inputs.shape[0], N_input)).to(device)\n","      if model_type == 'tcn':\n","        inputs = inputs.to(device) \n","      targets = targets.to(device)\n","      optimizer.zero_grad()\n","      outputs = model(inputs)\n","      targets, outputs = format_variables(targets, outputs, batch_size)\n","      if loss_type == 'soft_dtw':\n","        loss_sdtw = criterion(outputs, targets)\n","        loss = loss_sdtw.mean()\n","      else:\n","        loss = criterion(outputs, targets)\n","      loss.backward()\n","      optimizer.step()\n","      running_train_loss.append(loss.item())\n","    train_epoch_loss = np.mean(running_train_loss)\n","    loss_history['train loss'].append(train_epoch_loss)\n","    print('{} Loss: {:.5f}'.format(phase, train_epoch_loss))\n","\n","    # Evaluating\n","    phase = 'val' \n","    model.eval()\n","    running_test_loss = []\n","    with torch.no_grad():\n","      for inputs, targets in dataloaders[phase]:\n","        if model_type == 'mlp':\n","          inputs = inputs.reshape((inputs.shape[0], N_input)).to(device)\n","        if model_type == 'tcn':\n","          inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(inputs)\n","        targets, outputs = format_variables(targets, outputs, batch_size)\n","        if loss_type == 'soft_dtw':\n","          loss_sdtw = criterion(outputs, targets)\n","          loss = loss_sdtw.mean()\n","        else:\n","          loss = criterion(outputs, targets)\n","        running_test_loss.append(loss.item())\n","    test_epoch_loss = np.mean(running_test_loss)\n","    if test_epoch_loss < best_loss:\n","      best_loss = test_epoch_loss\n","      best_model_wts = copy.deepcopy(model.state_dict()) \n","      torch.save(model.state_dict(), f\"stored_model/{title}.pt\")\n","      es = 0\n","    else:\n","      es += 1\n","      print(\"Counter {} of 20\".format(es))\n","      if es > 20:\n","        print(\"Early stopping with best_validation_loss: \", best_loss)\n","        break\n","    loss_history['test loss'].append(test_epoch_loss)\n","    print('{} Loss: {:.5f}'.format(phase, test_epoch_loss))\n","    print('='*60)\n","  time_elapsed = time.time() - since\n","  print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","  print('Best validation Loss: {:5f}'.format(best_loss))\n","\n","  # load best model weights\n","  model.load_state_dict(best_model_wts)\n","  if loss_type == 'dtw_surrogate':\n","    return model, loss_history, criterion\n","  else:\n","    return model, loss_history"],"metadata":{"id":"Hg5b9qr8XS7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference(model, dataloaders, criterion, batch_size, loss_type='dtw_surrogate'):\n","  record = Record()\n","  losses_mse, losses_dtw  = [], []\n","  model = model.to(device)\n","  model.eval() \n","  running_loss = [] \n","  phase = 'test'\n","  with torch.no_grad():\n","    for inputs, targets in dataloaders[phase]:\n","      if model_type == 'mlp':\n","          inputs = inputs.reshape((inputs.shape[0], N_input)).to(device)\n","      if model_type == 'tcn':\n","        inputs = inputs.to(device) \n","      targets = targets.to(device)\n","      outputs = model(inputs)\n","      targets, outputs = format_variables(targets, outputs, batch_size)\n","      if loss_type == 'soft_dtw':\n","        loss_sdtw = criterion(outputs, targets)\n","        loss = loss_sdtw.mean()\n","      else:\n","        loss = criterion(outputs, targets)\n","      # statistics\n","      running_loss.append(loss.item())\n","      # compute metrics\n","      metric_mse, metric_dtw = record.update(outputs, targets)\n","      losses_mse.append(metric_mse)\n","      losses_dtw.append(metric_dtw)\n","  epoch_loss = np.mean(running_loss) \n","  print('For Seed:', seed)\n","  print('{} Loss: {:.5f}'.format(phase, epoch_loss))\n","  test_mse_metric = np.mean(losses_mse)\n","  test_dtw_metric = np.mean(losses_dtw)\n","  print('Metric MSE: {:.5f}, Metric DTW: {:.5f}'.format(test_mse_metric, test_dtw_metric))\n","  print('='*60)\n","  return epoch_loss, test_mse_metric, test_dtw_metric"],"metadata":{"id":"4nBIosdJXU4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","def train_model_surrogate(model, model_pred, dataloaders, optimizer, batch_size, num_epochs=25):\n","  since = time.time()\n","  loss_history = {'train loss':list(), 'test loss':list()} \n","  best_loss = float(\"inf\")\n","  best_model_wts = copy.deepcopy(model.state_dict())\n","  title =  f\"{data_set}_surrogate_model\"\n","  model = model.to(device)\n","  model_pred = model_pred.to(device)\n","  model_pred.eval()\n","\n","  for epoch in range(num_epochs):\n","    print('\\n Epoch {}/{}'.format(epoch, num_epochs - 1))\n","    print('-' * 10)\n","    # Training\n","    phase = 'train'\n","    model.train()\n","    running_train_loss = []\n","    for inputs, targets in dataloaders[phase]:\n","      if model_type == 'mlp':\n","          inputs = inputs.reshape((inputs.shape[0], N_input)).to(device)\n","      if model_type == 'tcn':\n","        inputs = inputs.to(device) \n","      targets = targets.to(device)\n","      optimizer.zero_grad()\n","      outputs = model_pred(inputs)\n","      targets, outputs = format_variables(targets, outputs, batch_size)\n","      loss_dtw_true = 0\n","      for k in range(batch_size):\n","        target_k_cpu = targets[k,:,0:1].view(-1).detach().cpu().numpy()\n","        output_k_cpu = outputs[k,:,0:1].view(-1).detach().cpu().numpy()\n","        sim = dtw(target_k_cpu, output_k_cpu)\n","        loss_dtw_true += sim\n","      loss_dtw_true = torch.tensor((loss_dtw_true/batch_size), dtype=torch.float32)\n","      loss_dtw_hat = model(targets, outputs)\n","      loss = MetaLoss(loss_dtw_hat.cpu(), loss_dtw_true) \n","      loss.backward()\n","      optimizer.step()\n","      train_loss = loss.item()\n","      running_train_loss.append(train_loss)\n","    train_epoch_loss = np.mean(running_train_loss)\n","    loss_history['train loss'].append(train_epoch_loss)\n","    print('{} Loss: {:.5f}'.format(phase, train_epoch_loss))\n","\n","    # Evaluating\n","    phase = 'val'\n","    model.eval()\n","    running_test_loss = []\n","    with torch.no_grad():\n","      for inputs, targets in dataloaders[phase]:\n","        if model_type == 'mlp':\n","          inputs = inputs.reshape((inputs.shape[0], N_input)).to(device)\n","        if model_type == 'tcn':\n","          inputs = inputs.to(device) \n","        targets = targets.to(device)\n","        outputs = model_pred(inputs)\n","        targets, outputs = format_variables(targets, outputs, batch_size)\n","        loss_dtw_true = 0\n","        for k in range(batch_size):\n","          target_k_cpu = targets[k,:,0:1].view(-1).detach().cpu().numpy()\n","          output_k_cpu = outputs[k,:,0:1].view(-1).detach().cpu().numpy()\n","          sim = dtw(target_k_cpu, output_k_cpu)\n","          loss_dtw_true += sim\n","        loss_dtw_true = torch.tensor((loss_dtw_true/batch_size), dtype=torch.float32)\n","        loss_dtw_hat = model(targets, outputs)\n","        loss = MetaLoss(loss_dtw_hat.cpu(), loss_dtw_true) \n","        test_loss = loss.item()\n","        running_test_loss.append(test_loss)\n","    test_epoch_loss = np.mean(running_test_loss)\n","    print('{} Loss: {:.5f}'.format(phase, test_epoch_loss))\n","    if test_epoch_loss < best_loss:\n","      best_loss = test_epoch_loss\n","      best_model_wts = copy.deepcopy(model.state_dict()) \n","      torch.save(model.state_dict(), f\"stored_model/{title}.pt\")\n","      es = 0\n","    else:\n","      es += 1\n","      print(\"Counter {} of 20\".format(es))\n","      if es > 20:\n","        print(\"Early stopping with best_validation_loss: \", best_loss)\n","        break\n","    loss_history['test loss'].append(test_epoch_loss)\n","    print('='*60)\n","\n","  time_elapsed = time.time() - since\n","  print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","  print('Best validation Loss: {:5f}'.format(best_loss))\n","\n","  # load best model weights\n","  model.load_state_dict(best_model_wts)\n","  return model, loss_history, model_pred"],"metadata":{"id":"IcM721ryXXvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimizer_run_surrogate():\n","  k = 10\n","  # step-1: pre-train FM with MSE\n","  if model_type == 'mlp':\n","    model_pred = MLP(N_input, N_output, hidden_units=128)\n","  if model_type == 'tcn':\n","    model_pred = TCN(N_input, N_output, ks=4)\n","  title = f\"{data_set}_prediction_model_mse_{model_type}\"\n","  model_pred.load_state_dict(torch.load(f\"stored_model/{title}.pt\"))\n","\n","  # step-2: pre-train SM with FM\n","  model_loss = SurrogateNetwork(data_set, batch_size=batch_size)\n","  model_pred = set_parameter_requires_grad_false(model_pred)\n","  optimizer = optim.Adam(model_loss.parameters())\n","  model_loss, loss_list, model_pred =  train_model_surrogate(model_loss, model_pred, dataloaders_dict, optimizer, batch_size, num_epochs=50)\n","  plot_losses(loss_list['train loss'], loss_list['test loss'], f\"{data_set}_pre_train_surrogate_losses_{model_type}\", 'train_loss', 'validation_loss', 'Loss plot')\n"," \n","  # step-3: train FM with SM\n","  for epoch in range(k):\n","    print('Iteration:', epoch)\n","    print('='*60)\n","    # train FM for alpha-steps\n","    print('Setting requires_grad to True for all modules in FM')\n","    model_pred = set_parameter_requires_grad_true(model_pred) \n","    optimizer = torch.optim.Adam(model_pred.parameters())\n","    print('Setting requires_grad False for LM')\n","    criterion = set_parameter_requires_grad_false(model_loss) \n","    model_pred, loss_list, model_loss = train_model(model_pred, dataloaders_dict, criterion, optimizer, batch_size, num_epochs=50, loss_type='dtw_surrogate')\n","\n","    # train LM for beta-steps\n","    print('Setting requires_grad False for FM')\n","    model_pred = set_parameter_requires_grad_false(model_pred)\n","    print('Setting requires_grad True for LM')\n","    model_loss = set_parameter_requires_grad_true(model_loss)\n","    optimizer = optim.Adam(model_loss.parameters()) \n","    model_loss, loss_list, model_pred =  train_model_surrogate(model_loss, model_pred, dataloaders_dict, optimizer, batch_size, num_epochs=50)\n","    print('#'*60)\n","\n","  # step-4 inference\n","  print('Setting requires_grad to True for all layers in FM')\n","  model_pred = set_parameter_requires_grad_true(model_pred)\n","  criterion = set_parameter_requires_grad_false(model_loss)\n","  test_loss, test_mse_metric, test_dtw_metric = inference(model_pred, dataloaders_dict, criterion, batch_size, loss_type='dtw_surrogate')\n","\n","  seedperf = {}\n","  seedperf['Seed'] = data_set + '_' + str(seed)\n","  # store the results\n","  seedperf['output_timestamp'] = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n","  seedperf['loss_type'], seedperf['test_loss'], seedperf['mse_metric'], seedperf['dtw_metric'] = 'dtw_surrogate', test_loss, test_mse_metric, test_dtw_metric\n","  json.dump(seedperf, open(\"stored_model/results_tcn.json\", \"a\"), indent=4)\n","  print('Results stored!')\n"],"metadata":{"id":"Zf0F0V3HXhJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def optimizer_run(loss_type):\n","  seedperf = {}\n","  seedperf['Seed'] = data_set + '_' + str(seed)\n","  # initialize the prediction model\n","  if model_type == 'mlp':\n","    model_pred = MLP(N_input, N_output, hidden_units=128)\n","  if model_type == 'tcn':\n","    model_pred = TCN(N_input, N_output, ks=4)\n","  if loss_type == 'mse':\n","    criterion = nn.MSELoss()\n","  if loss_type == 'soft_dtw':\n","    criterion = SoftDTW(use_cuda=True, gamma=0.01)\n","  optimizer = optim.Adam(model_pred.parameters())\n","  model_pred, loss_list = train_model(model_pred, dataloaders_dict, criterion, optimizer, batch_size, loss_type, num_epochs=25) \n","  #plot_values_1, plot_values_2, plt_name, label_1, label_2, plt_title\n","  plot_losses(loss_list['train loss'], loss_list['test loss'], f\"{data_set}_train_prediction_losses_for_{loss_type}_{seed}_{model_type}\", 'train_loss', 'validation_loss', f\"{data_set}_{loss_type}_{seed}\")\n","  test_loss, test_mse_metric, test_dtw_metric = inference(model_pred, dataloaders_dict, criterion, batch_size, loss_type)\n","  \n","  # store the results\n","  seedperf['output_timestamp'] = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n","  seedperf['loss_type'], seedperf['test_loss'], seedperf['mse_metric'], seedperf['dtw_metric'] = loss_type, test_loss, test_mse_metric, test_dtw_metric\n","  json.dump(seedperf, open(\"stored_model/results_tcn.json\", \"a\"), indent=4)\n","  print('Results stored!')"],"metadata":{"id":"ljMSN-KoXmrl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Start the execution from here."],"metadata":{"id":"M37o60Q9e8wF"}},{"cell_type":"code","source":["if loss == 'dtw_surrogate':\n","  optimizer_run_surrogate()\n","else:\n","  optimizer_run(loss_type=loss)"],"metadata":{"id":"4GH3f809ez5g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Plot the results here."],"metadata":{"id":"bSvk2l0kfSXy"}},{"cell_type":"code","source":["def visualize_results(nets, dataset):\n","  loss_list = ['mse', 'soft_dtw', 'dtw_surrogate']\n","  gen_test = iter(dataloaders_dict['test'])\n","  test_inputs, test_targets = next(gen_test)\n","\n","  test_inputs  = torch.tensor(test_inputs, dtype=torch.float32).to(device)\n","  test_targets = torch.tensor(test_targets, dtype=torch.float32).to(device)\n","  print(test_inputs.shape, test_targets.shape)\n","\n","  for ind in range(1,5):\n","    plt.figure()\n","    plt.rcParams['figure.figsize'] = (17.0,5.0)  \n","    \n","    k = 1\n","    i=0\n","    for net in nets:\n","      loss_type = loss_list[i]\n","      net = net.to(device)\n","      if model_type == 'mlp':\n","        inputs = test_inputs.reshape((test_inputs.shape[0], N_input)).to(device)\n","      if model_type == 'tcn':\n","        inputs = test_inputs.to(device)\n","      pred = net(inputs).to(device)\n","\n","      print(test_inputs.shape, test_targets.shape, pred.shape)\n","      test_targets, pred = format_variables(test_targets, pred, batch_size)\n","      input = test_inputs.detach().cpu().numpy()[ind,:,:]\n","      target = test_targets.detach().cpu().numpy()[ind,:,:]\n","      preds = pred.detach().cpu().numpy()[ind,:,:]\n","\n","      plt.subplot(1,3,k)\n","      plt.plot(range(0,N_input) ,input,label='input',linewidth=3)\n","      plt.plot(range(N_input-1,N_input+N_output), np.concatenate([ input[N_input-1:N_input], target ]) ,label='target',linewidth=3)   \n","      plt.plot(range(N_input-1,N_input+N_output),  np.concatenate([ input[N_input-1:N_input], preds ])  ,label='prediction',linewidth=3)       \n","      #plt.xticks(range(0,40,2))\n","      plt.legend()\n","      plt.title(loss_type)\n","      k = k+1\n","      i = i+1\n","    fig1 = plt.gcf()\n","    plt.show()\n","    plt.draw()\n","    fig1.savefig(f'plots/{model_type}_{dataset}_{ind}.png')"],"metadata":{"id":"X7_Uae22fW8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize results\n","#mse\n","title = f\"{data_set}_prediction_model_mse_{model_type}\"\n","net_mse =  TCN(N_input, N_output, ks=4)\n","net_mse.load_state_dict(torch.load(f\"stored_model/{title}.pt\"))\n","#soft_dtw\n","title = f\"{data_set}_prediction_model_soft_dtw_{model_type}\"\n","net_soft_dtw =  TCN(N_input, N_output, ks=4)\n","net_soft_dtw.load_state_dict(torch.load(f\"stored_model/{title}.pt\"))\n","#dtw_surrogate\n","title = f\"{data_set}_prediction_model_dtw_surrogate_{model_type}\"\n","net_s_dtw =  TCN(N_input, N_output, ks=4)\n","net_s_dtw.load_state_dict(torch.load(f\"stored_model/{title}.pt\"))\n","\n","nets = [net_mse, net_soft_dtw, net_s_dtw]\n","visualize_results(nets, data_set)"],"metadata":{"id":"3f8sYtcrfhtA"},"execution_count":null,"outputs":[]}]}